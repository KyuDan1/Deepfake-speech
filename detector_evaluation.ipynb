{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speaker-Invariant Deepfake Detector Evaluation\n",
    "\n",
    "이 노트북은 다양한 n_speaker_components 값에 대해 deepfake detector의 성능을 평가합니다.\n",
    "\n",
    "## 실험 목표\n",
    "1. n_speaker_components = 1, 5, 10, 16에 대해 모델 학습\n",
    "2. Train/Test 데이터셋 분할\n",
    "3. 각 설정별 정확도, Precision, Recall, F1-Score 측정\n",
    "4. 결과 비교 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from transformers import WavLMModel, Wav2Vec2FeatureExtractor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 설정\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SpeakerInvariantDetector 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeakerInvariantDetector:\n",
    "    def __init__(self, model_name=\"microsoft/wavlm-large\", device=None):\n",
    "        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        print(f\"Loading WavLM model ({self.device})...\")\n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "        self.model = WavLMModel.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # 투영 행렬 (Speaker info 제거용)과 분류기\n",
    "        self.projection_matrix = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.classifier = LogisticRegression(random_state=42, solver='liblinear', max_iter=1000)\n",
    "        self.pca = None\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def _extract_feature(self, audio_input):\n",
    "        \"\"\"내부용: Wav 파일 경로 또는 numpy array에서 WavLM feature 추출\"\"\"\n",
    "        try:\n",
    "            # 경로인 경우 로드, 아니면 그대로 사용\n",
    "            if isinstance(audio_input, (str, Path)):\n",
    "                audio, _ = librosa.load(str(audio_input), sr=16000, mono=True)\n",
    "            else:\n",
    "                audio = audio_input # 이미 numpy array라고 가정\n",
    "\n",
    "            inputs = self.feature_extractor(\n",
    "                audio, sampling_rate=16000, return_tensors=\"pt\", padding=True\n",
    "            )\n",
    "            input_values = inputs.input_values.to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_values)\n",
    "            \n",
    "            # (Batch, Time, Dim) -> Mean Pooling -> (Dim,)\n",
    "            pooled_features = outputs.last_hidden_state.mean(dim=1).squeeze(0).cpu().numpy()\n",
    "            return pooled_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Feature extraction error: {e}\")\n",
    "            return None\n",
    "\n",
    "    def fit(self, audio_paths, labels, speaker_ids, n_speaker_components=10):\n",
    "        \"\"\"\n",
    "        모델 학습 함수 (Projection Matrix 계산 + Classifier 학습)\n",
    "        \n",
    "        Args:\n",
    "            audio_paths: 오디오 파일 경로 리스트\n",
    "            labels: 0 (Real), 1 (Fake) 등의 레이블 리스트\n",
    "            speaker_ids: 각 오디오의 화자 ID 리스트 (Speaker Subspace 계산용)\n",
    "            n_speaker_components: 제거할 화자 정보 차원 수 (PC 개수)\n",
    "        \"\"\"\n",
    "        print(\"Extracting features for training...\")\n",
    "        X_raw = []\n",
    "        y = []\n",
    "        spk_map = {} # {speaker_id: [indices]}\n",
    "        \n",
    "        # 1. Feature Extraction\n",
    "        for idx, (path, label, spk) in enumerate(tqdm(list(zip(audio_paths, labels, speaker_ids)), desc=\"Extracting features\")):\n",
    "            feat = self._extract_feature(path)\n",
    "            if feat is not None:\n",
    "                X_raw.append(feat)\n",
    "                y.append(label)\n",
    "                if spk not in spk_map: spk_map[spk] = []\n",
    "                spk_map[spk].append(len(X_raw) - 1)\n",
    "        \n",
    "        X_raw = np.array(X_raw)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        # 2. Scaling\n",
    "        print(\"Scaling features...\")\n",
    "        X_scaled = self.scaler.fit_transform(X_raw)\n",
    "        \n",
    "        # 3. Compute Speaker Subspace (PCA on Speaker Centroids)\n",
    "        print(f\"Computing Speaker Subspace (removing top {n_speaker_components} components)...\")\n",
    "        speaker_centroids = []\n",
    "        for spk, indices in spk_map.items():\n",
    "            # 해당 화자의 모든 발화 평균 계산\n",
    "            centroid = np.mean(X_scaled[indices], axis=0)\n",
    "            speaker_centroids.append(centroid)\n",
    "        \n",
    "        speaker_centroids = np.array(speaker_centroids)\n",
    "        \n",
    "        # 화자 평균들에 대해 PCA 수행하여 주요 \"화자 방향(Basis)\" 찾기\n",
    "        self.pca = PCA(n_components=n_speaker_components)\n",
    "        self.pca.fit(speaker_centroids)\n",
    "        \n",
    "        # Orthogonal Projection Matrix 생성: P_perp = I - U @ U.T\n",
    "        # U: Speaker Basis Vectors (n_features, n_components)\n",
    "        U = self.pca.components_.T \n",
    "        I = np.eye(U.shape[0])\n",
    "        self.projection_matrix = I - (U @ U.T)\n",
    "        \n",
    "        # 4. Project Features (Remove Speaker Info)\n",
    "        # X_proj = X @ P_perp\n",
    "        X_projected = X_scaled @ self.projection_matrix\n",
    "        \n",
    "        # 5. Train Simple Classifier (Logistic Regression)\n",
    "        print(\"Training Decision Boundary (Logistic Regression)...\")\n",
    "        self.classifier.fit(X_projected, y)\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        # 학습 결과 요약\n",
    "        acc = self.classifier.score(X_projected, y)\n",
    "        print(f\"Training Complete. Accuracy on Train Set: {acc:.4f}\")\n",
    "        \n",
    "        return acc\n",
    "\n",
    "    def predict(self, audio_path):\n",
    "        \"\"\"\n",
    "        Inference 함수: Wav -> Feature -> Scale -> Project -> Predict\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model has not been fitted yet. Call fit() first.\")\n",
    "        \n",
    "        # 1. Extract\n",
    "        feat = self._extract_feature(audio_path)\n",
    "        if feat is None: return None\n",
    "        \n",
    "        # 2. Scale\n",
    "        # (1, Dim) 형태로 변환\n",
    "        feat = feat.reshape(1, -1)\n",
    "        feat_scaled = self.scaler.transform(feat)\n",
    "        \n",
    "        # 3. Project (Remove Speaker Info)\n",
    "        # 수학적으로: x_new = x_old @ (I - UU^T)\n",
    "        feat_projected = feat_scaled @ self.projection_matrix\n",
    "        \n",
    "        # 4. Predict\n",
    "        prob = self.classifier.predict_proba(feat_projected)[0]\n",
    "        pred_label = self.classifier.predict(feat_projected)[0]\n",
    "        \n",
    "        return {\n",
    "            \"label\": pred_label,          # 예측 클래스\n",
    "            \"probability\": prob,          # [Prob_Class0, Prob_Class1]\n",
    "            \"feature_vector\": feat_projected # 투영된 벡터 (시각화용)\n",
    "        }\n",
    "    \n",
    "    def predict_batch(self, audio_paths):\n",
    "        \"\"\"\n",
    "        배치 추론 함수 (효율성 향상)\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        probabilities = []\n",
    "        \n",
    "        for path in tqdm(audio_paths, desc=\"Predicting\"):\n",
    "            result = self.predict(path)\n",
    "            if result is not None:\n",
    "                predictions.append(result['label'])\n",
    "                probabilities.append(result['probability'])\n",
    "            else:\n",
    "                predictions.append(-1)  # Error marker\n",
    "                probabilities.append([0, 0])\n",
    "        \n",
    "        return np.array(predictions), np.array(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libri_dataframe import build_librispeech_dataframe\n",
    "\n",
    "LIBRISPEECH_ROOT = \"./my_raw_audio/LibriSpeech\"\n",
    "LIBRISPEECH_SUBSET = \"test-clean\"\n",
    "\n",
    "dataframe = build_librispeech_dataframe(\n",
    "    librispeech_root=LIBRISPEECH_ROOT,\n",
    "    subset=LIBRISPEECH_SUBSET,\n",
    ")\n",
    "\n",
    "# 처음 1000개만 사용\n",
    "dataframe_10 = dataframe[:1000]\n",
    "print(f\"Total samples: {len(dataframe_10)}\")\n",
    "print(f\"Number of speakers: {dataframe_10['speaker_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 준비 (Real + Fake)\n",
    "all_paths = []\n",
    "all_labels = []\n",
    "all_speakers = []\n",
    "\n",
    "print(\"Preparing dataset...\")\n",
    "for row in dataframe_10.to_dict('records'):\n",
    "    # Real Data\n",
    "    real_path = Path(row['audio_path'])\n",
    "    if real_path.exists():\n",
    "        all_paths.append(str(real_path))\n",
    "        all_labels.append(0)  # 0 for Real\n",
    "        all_speakers.append(row['speaker_id'])\n",
    "    \n",
    "    # Fake Data (Gen 2)\n",
    "    base_name = real_path.stem\n",
    "    fake_path = Path(f\"generated_results/speaker_libri_transcript_{base_name}.wav\")\n",
    "    if fake_path.exists():\n",
    "        all_paths.append(str(fake_path))\n",
    "        all_labels.append(1)  # 1 for Fake\n",
    "        all_speakers.append(row['speaker_id'])  # 같은 화자 ID\n",
    "\n",
    "print(f\"Total samples collected: {len(all_paths)}\")\n",
    "print(f\"Real samples: {sum(1 for l in all_labels if l == 0)}\")\n",
    "print(f\"Fake samples: {sum(1 for l in all_labels if l == 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train/Test Split\n",
    "\n",
    "데이터를 80% Train, 20% Test로 분할합니다.\n",
    "**중요**: Speaker가 Train/Test에 섞이지 않도록 stratify를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split (80/20)\n",
    "X_train_paths, X_test_paths, y_train, y_test, spk_train, spk_test = train_test_split(\n",
    "    all_paths, all_labels, all_speakers,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=all_labels  # Label 비율 유지\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {len(X_train_paths)} samples\")\n",
    "print(f\"  Real: {sum(1 for l in y_train if l == 0)}, Fake: {sum(1 for l in y_train if l == 1)}\")\n",
    "print(f\"\\nTest set: {len(X_test_paths)} samples\")\n",
    "print(f\"  Real: {sum(1 for l in y_test if l == 0)}, Fake: {sum(1 for l in y_test if l == 1)}\")\n",
    "print(f\"\\nUnique speakers in train: {len(set(spk_train))}\")\n",
    "print(f\"Unique speakers in test: {len(set(spk_test))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 실험: n_speaker_components = 1, 5, 10, 16\n",
    "\n",
    "각 n값에 대해:\n",
    "1. 모델 학습 (Train set)\n",
    "2. 예측 수행 (Test set)\n",
    "3. 성능 지표 계산 (Accuracy, Precision, Recall, F1)\n",
    "4. Confusion Matrix 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실험할 n_speaker_components 값들\n",
    "n_components_list = [1, 5, 10, 16]\n",
    "\n",
    "# 결과 저장용 딕셔너리\n",
    "results = {\n",
    "    'n_components': [],\n",
    "    'train_accuracy': [],\n",
    "    'test_accuracy': [],\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'f1_score': [],\n",
    "    'confusion_matrix': [],\n",
    "    'predictions': [],\n",
    "    'probabilities': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WavLM 모델은 한 번만 로드 (공유)\n",
    "print(\"Initializing WavLM model (this will be shared across experiments)...\")\n",
    "base_detector = SpeakerInvariantDetector(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실험 루프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in n_components_list:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Experiment: n_speaker_components = {n}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # 새로운 detector 인스턴스 생성 (매번 fresh start)\n",
    "    detector = SpeakerInvariantDetector(device=device)\n",
    "    \n",
    "    # 학습\n",
    "    print(\"\\n[1/3] Training...\")\n",
    "    train_acc = detector.fit(X_train_paths, y_train, spk_train, n_speaker_components=n)\n",
    "    \n",
    "    # 테스트 예측\n",
    "    print(\"\\n[2/3] Testing...\")\n",
    "    y_pred, y_prob = detector.predict_batch(X_test_paths)\n",
    "    \n",
    "    # 성능 평가\n",
    "    print(\"\\n[3/3] Evaluating...\")\n",
    "    test_acc = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='binary')\n",
    "    recall = recall_score(y_test, y_pred, average='binary')\n",
    "    f1 = f1_score(y_test, y_pred, average='binary')\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # 결과 저장\n",
    "    results['n_components'].append(n)\n",
    "    results['train_accuracy'].append(train_acc)\n",
    "    results['test_accuracy'].append(test_acc)\n",
    "    results['precision'].append(precision)\n",
    "    results['recall'].append(recall)\n",
    "    results['f1_score'].append(f1)\n",
    "    results['confusion_matrix'].append(cm)\n",
    "    results['predictions'].append(y_pred)\n",
    "    results['probabilities'].append(y_prob)\n",
    "    \n",
    "    # 결과 출력\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Results for n = {n}:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Test Accuracy:  {test_acc:.4f}\")\n",
    "    print(f\"Precision:      {precision:.4f}\")\n",
    "    print(f\"Recall:         {recall:.4f}\")\n",
    "    print(f\"F1-Score:       {f1:.4f}\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(f\"\\n{classification_report(y_test, y_pred, target_names=['Real', 'Fake'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 결과 요약 및 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 결과 테이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 DataFrame으로 정리\n",
    "results_df = pd.DataFrame({\n",
    "    'n_components': results['n_components'],\n",
    "    'Train Accuracy': results['train_accuracy'],\n",
    "    'Test Accuracy': results['test_accuracy'],\n",
    "    'Precision': results['precision'],\n",
    "    'Recall': results['recall'],\n",
    "    'F1-Score': results['f1_score']\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Summary of All Experiments\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 성능 지표 비교 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능 지표 시각화\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# (0, 0): Train vs Test Accuracy\n",
    "ax = axes[0, 0]\n",
    "ax.plot(results['n_components'], results['train_accuracy'], marker='o', linewidth=2, markersize=8, label='Train Accuracy')\n",
    "ax.plot(results['n_components'], results['test_accuracy'], marker='s', linewidth=2, markersize=8, label='Test Accuracy')\n",
    "ax.set_xlabel('n_speaker_components', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Train vs Test Accuracy', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, linestyle='--', alpha=0.5)\n",
    "ax.set_xticks(results['n_components'])\n",
    "\n",
    "# (0, 1): Precision\n",
    "ax = axes[0, 1]\n",
    "ax.plot(results['n_components'], results['precision'], marker='o', linewidth=2, markersize=8, color='green')\n",
    "ax.set_xlabel('n_speaker_components', fontsize=12)\n",
    "ax.set_ylabel('Precision', fontsize=12)\n",
    "ax.set_title('Precision (Fake Detection)', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, linestyle='--', alpha=0.5)\n",
    "ax.set_xticks(results['n_components'])\n",
    "\n",
    "# (1, 0): Recall\n",
    "ax = axes[1, 0]\n",
    "ax.plot(results['n_components'], results['recall'], marker='o', linewidth=2, markersize=8, color='orange')\n",
    "ax.set_xlabel('n_speaker_components', fontsize=12)\n",
    "ax.set_ylabel('Recall', fontsize=12)\n",
    "ax.set_title('Recall (Fake Detection)', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, linestyle='--', alpha=0.5)\n",
    "ax.set_xticks(results['n_components'])\n",
    "\n",
    "# (1, 1): F1-Score\n",
    "ax = axes[1, 1]\n",
    "ax.plot(results['n_components'], results['f1_score'], marker='o', linewidth=2, markersize=8, color='red')\n",
    "ax.set_xlabel('n_speaker_components', fontsize=12)\n",
    "ax.set_ylabel('F1-Score', fontsize=12)\n",
    "ax.set_title('F1-Score (Fake Detection)', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, linestyle='--', alpha=0.5)\n",
    "ax.set_xticks(results['n_components'])\n",
    "\n",
    "fig.suptitle('Speaker-Invariant Detector Performance Comparison', fontsize=18, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Confusion Matrix 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix 시각화\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "for idx, (n, cm) in enumerate(zip(results['n_components'], results['confusion_matrix'])):\n",
    "    ax = axes[idx]\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, \n",
    "                xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'],\n",
    "                cbar=True, square=True)\n",
    "    ax.set_xlabel('Predicted', fontsize=11)\n",
    "    ax.set_ylabel('Actual', fontsize=11)\n",
    "    ax.set_title(f'n = {n}\\nAcc: {results[\"test_accuracy\"][idx]:.3f}', fontsize=12, fontweight='bold')\n",
    "\n",
    "fig.suptitle('Confusion Matrices Comparison', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 최적 n_components 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최고 성능을 보인 n_components 찾기\n",
    "best_idx = np.argmax(results['test_accuracy'])\n",
    "best_n = results['n_components'][best_idx]\n",
    "best_acc = results['test_accuracy'][best_idx]\n",
    "best_f1 = results['f1_score'][best_idx]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Best Configuration\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"n_speaker_components: {best_n}\")\n",
    "print(f\"Test Accuracy:        {best_acc:.4f}\")\n",
    "print(f\"F1-Score:             {best_f1:.4f}\")\n",
    "print(f\"Precision:            {results['precision'][best_idx]:.4f}\")\n",
    "print(f\"Recall:               {results['recall'][best_idx]:.4f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 분석 및 결론\n",
    "\n",
    "### 예상 결과:\n",
    "\n",
    "1. **n = 1 (매우 약한 제거)**\n",
    "   - 화자 정보가 많이 남아 있어 overfitting 가능성\n",
    "   - Train accuracy는 높지만 Test accuracy는 낮을 수 있음\n",
    "\n",
    "2. **n = 5~10 (적절한 제거)**\n",
    "   - 화자 정보는 충분히 제거하면서 Fake 탐지 정보는 보존\n",
    "   - 가장 균형 잡힌 성능 기대\n",
    "\n",
    "3. **n = 16 (강한 제거)**\n",
    "   - 화자 정보는 거의 제거되지만\n",
    "   - Fake 탐지에 필요한 정보까지 손실될 가능성\n",
    "\n",
    "### 관찰 포인트:\n",
    "- Train/Test accuracy gap이 작을수록 일반화 성능이 좋음\n",
    "- Precision과 Recall의 균형 (F1-Score로 확인)\n",
    "- Confusion Matrix에서 False Positive/Negative 비율"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 결과 저장 (선택 사항)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 CSV로 저장\n",
    "results_df.to_csv('detector_evaluation_results.csv', index=False)\n",
    "print(\"Results saved to 'detector_evaluation_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 10. CLI 사용법\n\n저장된 모델은 커맨드라인에서도 사용할 수 있습니다:\n\n```bash\n# 단일 파일 예측\npython inference.py --audio_path /path/to/audio.wav --model_path ./models/detector_n10.pkl\n\n# 출력 예시:\n# Prediction: FAKE\n# Confidence: 87.34%\n# Detailed Probabilities:\n#   Real: 0.1266\n#   Fake: 0.8734\n```\n\n### Python 코드에서 사용:\n\n```python\nfrom inference import DeepfakeDetector\n\n# 모델 로드\ndetector = DeepfakeDetector(model_path=\"./models/detector_n10.pkl\")\n\n# 단일 파일 예측\nresult = detector.predict(\"new_audio.wav\")\nprint(f\"Is Synthetic: {result['is_fake']}\")\nprint(f\"Confidence: {result['confidence']:.2%}\")\n\n# 배치 예측\naudio_files = [\"audio1.wav\", \"audio2.wav\", \"audio3.wav\"]\nresults = detector.predict_batch(audio_files)\nfor audio_file, result in zip(audio_files, results):\n    print(f\"{audio_file}: {'FAKE' if result['is_fake'] else 'REAL'} ({result['confidence']:.2%})\")\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 테스트 셋에서 몇 개 샘플 선택하여 inference 테스트\ntest_samples = X_test_paths[:5]  # 처음 5개 샘플\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"Inference Examples\")\nprint(\"=\"*80)\n\nfor i, audio_path in enumerate(test_samples, 1):\n    result = inference_detector.predict(audio_path)\n    \n    actual_label = y_test[X_test_paths.index(audio_path)]\n    actual_str = \"REAL\" if actual_label == 0 else \"FAKE\"\n    \n    print(f\"\\n[Sample {i}]\")\n    print(f\"File: {Path(audio_path).name}\")\n    print(f\"Actual:     {actual_str}\")\n    print(f\"Predicted:  {'FAKE' if result['is_fake'] else 'REAL'}\")\n    print(f\"Confidence: {result['confidence']:.2%}\")\n    print(f\"Probabilities: Real={result['probabilities']['real']:.4f}, Fake={result['probabilities']['fake']:.4f}\")\n    \n    # 정답 여부 표시\n    is_correct = (result['is_fake'] and actual_label == 1) or (not result['is_fake'] and actual_label == 0)\n    print(f\"Result: {'✅ CORRECT' if is_correct else '❌ WRONG'}\")\n\nprint(\"\\n\" + \"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# inference.py 모듈 import\nfrom inference import DeepfakeDetector\n\n# 저장된 모델 로드\nprint(\"Loading saved model for inference...\")\ninference_detector = DeepfakeDetector(model_path=model_save_path, device=device)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Inference 예제\n\n저장된 모델을 불러와서 새로운 오디오 파일에 대해 예측하는 방법을 보여줍니다.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 최고 성능 모델을 다시 학습하여 저장\nprint(f\"\\nRetraining best model (n={best_n}) for saving...\")\nprint(\"=\"*80)\n\nbest_detector = SpeakerInvariantDetector(device=device)\nbest_detector.fit(X_train_paths, y_train, spk_train, n_speaker_components=best_n)\n\n# 모델 저장\nmodel_save_path = f\"./models/detector_n{best_n}.pkl\"\nsave_trained_model(best_detector, model_save_path, best_n)\n\nprint(\"\\n✅ Best model has been saved!\")\nprint(f\"   Model path: {model_save_path}\")\nprint(f\"   n_speaker_components: {best_n}\")\nprint(f\"   Test Accuracy: {best_acc:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import pickle\n\ndef save_trained_model(detector, save_path, n_speaker_components, model_name=\"microsoft/wavlm-large\"):\n    \"\"\"\n    학습된 SpeakerInvariantDetector를 저장합니다.\n\n    Args:\n        detector: 학습된 SpeakerInvariantDetector 인스턴스\n        save_path: 저장할 파일 경로 (.pkl)\n        n_speaker_components: 사용한 speaker component 개수\n        model_name: 사용한 WavLM 모델 이름\n    \"\"\"\n    if not detector.is_fitted:\n        raise ValueError(\"Detector must be fitted before saving!\")\n\n    save_data = {\n        'scaler': detector.scaler,\n        'projection_matrix': detector.projection_matrix,\n        'classifier': detector.classifier,\n        'n_speaker_components': n_speaker_components,\n        'model_name': model_name,\n        'pca': detector.pca  # 추가 정보 (분석용)\n    }\n\n    # 디렉토리가 없으면 생성\n    Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n\n    with open(save_path, 'wb') as f:\n        pickle.dump(save_data, f)\n\n    print(f\"Model saved to {save_path}\")\n    print(f\"  n_speaker_components: {n_speaker_components}\")\n    print(f\"  Model name: {model_name}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8. 최고 성능 모델 저장\n\n최고 성능을 보인 모델을 저장하여 나중에 inference에 사용할 수 있습니다.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}