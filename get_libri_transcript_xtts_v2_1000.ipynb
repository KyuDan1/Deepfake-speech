{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéôÔ∏è XTTS-v2 Deepfake Speech Analysis\n",
        "\n",
        "LibriSpeech Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ XTTS-v2Î°ú ÏùåÏÑ±ÏùÑ ÏÉùÏÑ±ÌïòÍ≥†, Real vs Generated ÏùåÏÑ±ÏùÑ Î∂ÑÏÑùÌï©ÎãàÎã§.\n",
        "\n",
        "### Î∂ÑÏÑù ÎÇ¥Ïö©\n",
        "1. **Gradient Field Î∂ÑÏÑù**: Mel SpectrogramÏùò GradientÎ•º t-SNE, UMAPÏúºÎ°ú ÏãúÍ∞ÅÌôî\n",
        "2. **Whisper Encoder Î∂ÑÏÑù**: Whisper Î™®Îç∏Ïùò encoder featuresÎ•º UMAPÏúºÎ°ú ÏãúÍ∞ÅÌôî\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/mnt/fr20tb/jihwan/.conda/envs/tts/lib/python3.10/site-packages/umap/__init__.py:35: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  import pkg_resources\n",
            "/mnt/fr20tb/jihwan/.conda/envs/tts/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# ÎùºÏù¥Î∏åÎü¨Î¶¨ Import\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import soundfile as sf\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "import librosa\n",
        "import umap\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import TSNE\n",
        "from transformers import WhisperProcessor, WhisperModel\n",
        "import torchaudio\n",
        "\n",
        "# PyTorch 2.6+ weights_only Ïù¥Ïäà Ìï¥Í≤∞\n",
        "import torch.serialization\n",
        "torch.serialization.add_safe_globals([dict])\n",
        "# ÎòêÎäî Ï†ÑÏó≠Ï†ÅÏúºÎ°ú weights_only=False ÏÇ¨Ïö©ÌïòÎèÑÎ°ù ÏÑ§Ï†ï\n",
        "import functools\n",
        "_original_torch_load = torch.load\n",
        "torch.load = functools.partial(_original_torch_load, weights_only=False)\n",
        "\n",
        "from TTS.api import TTS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÅ OUTPUT_DIR: /mnt/ddn/jihwan/projects/code/Deepfake-speech/generated_results_XTTS_v2_Random\n",
            "üìÅ PROMPT_WAV_DIR: /mnt/ddn/jihwan/projects/code/Deepfake-speech/prompt_wav_files\n",
            "üìÅ FIGURE_DIR: /mnt/ddn/jihwan/projects/code/Deepfake-speech/analysis_figures_xtts_v2\n"
          ]
        }
      ],
      "source": [
        "# Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
        "N_SAMPLES = 1000\n",
        "LIBRISPEECH_ROOT = \"./my_librispeech/LibriSpeech\"\n",
        "LIBRISPEECH_SUBSET = \"test-clean\"\n",
        "OUTPUT_DIR = Path(f\"generated_results_XTTS_v2_{N_SAMPLES}\").resolve()\n",
        "PROMPT_WAV_DIR = Path(\"prompt_wav_files\").resolve()\n",
        "FIGURE_DIR = Path(f\"analysis_figures_xtts_v2_{N_SAMPLES}\").resolve()\n",
        "CUDA_DEVICE = \"0\"\n",
        "\n",
        "# Ìè¥Îçî ÏÉùÏÑ±\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(PROMPT_WAV_DIR, exist_ok=True)\n",
        "os.makedirs(FIGURE_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"üìÅ OUTPUT_DIR: {OUTPUT_DIR}\")\n",
        "print(f\"üìÅ PROMPT_WAV_DIR: {PROMPT_WAV_DIR}\")\n",
        "print(f\"üìÅ FIGURE_DIR: {FIGURE_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ Íµ¨Ï∂ï Î∞è ÏÉòÌîåÎßÅ ===\n",
            "Loaded 2620 utterances from my_librispeech/LibriSpeech/test-clean\n",
            "  - Speakers: 40\n",
            "  - Chapters: 87\n",
            "‚úÖ ÏÉòÌîåÎßÅ ÏôÑÎ£å: 100Í∞ú ÌååÏùº (ÌôîÏûê Ïàò: 37Î™Ö)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>speaker_id</th>\n",
              "      <th>chapter_id</th>\n",
              "      <th>utterance_id</th>\n",
              "      <th>transcript</th>\n",
              "      <th>audio_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7729</td>\n",
              "      <td>102255</td>\n",
              "      <td>7729-102255-0008</td>\n",
              "      <td>ALL THE TERRITORIAL DIGNITARIES WERE PRESENT G...</td>\n",
              "      <td>my_librispeech/LibriSpeech/test-clean/7729/102...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1221</td>\n",
              "      <td>135766</td>\n",
              "      <td>1221-135766-0004</td>\n",
              "      <td>THIS OUTWARD MUTABILITY INDICATED AND DID NOT ...</td>\n",
              "      <td>my_librispeech/LibriSpeech/test-clean/1221/135...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4507</td>\n",
              "      <td>16021</td>\n",
              "      <td>4507-16021-0032</td>\n",
              "      <td>HE MUST DESCEND WITH HIS HEART FULL OF CHARITY...</td>\n",
              "      <td>my_librispeech/LibriSpeech/test-clean/4507/160...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5105</td>\n",
              "      <td>28241</td>\n",
              "      <td>5105-28241-0019</td>\n",
              "      <td>NOTHING WAS TO BE DONE BUT TO PUT ABOUT AND RE...</td>\n",
              "      <td>my_librispeech/LibriSpeech/test-clean/5105/282...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8463</td>\n",
              "      <td>294825</td>\n",
              "      <td>8463-294825-0001</td>\n",
              "      <td>THIS REALITY BEGINS TO EXPLAIN THE DARK POWER ...</td>\n",
              "      <td>my_librispeech/LibriSpeech/test-clean/8463/294...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   speaker_id  chapter_id      utterance_id  \\\n",
              "0        7729      102255  7729-102255-0008   \n",
              "1        1221      135766  1221-135766-0004   \n",
              "2        4507       16021   4507-16021-0032   \n",
              "3        5105       28241   5105-28241-0019   \n",
              "4        8463      294825  8463-294825-0001   \n",
              "\n",
              "                                          transcript  \\\n",
              "0  ALL THE TERRITORIAL DIGNITARIES WERE PRESENT G...   \n",
              "1  THIS OUTWARD MUTABILITY INDICATED AND DID NOT ...   \n",
              "2  HE MUST DESCEND WITH HIS HEART FULL OF CHARITY...   \n",
              "3  NOTHING WAS TO BE DONE BUT TO PUT ABOUT AND RE...   \n",
              "4  THIS REALITY BEGINS TO EXPLAIN THE DARK POWER ...   \n",
              "\n",
              "                                          audio_path  \n",
              "0  my_librispeech/LibriSpeech/test-clean/7729/102...  \n",
              "1  my_librispeech/LibriSpeech/test-clean/1221/135...  \n",
              "2  my_librispeech/LibriSpeech/test-clean/4507/160...  \n",
              "3  my_librispeech/LibriSpeech/test-clean/5105/282...  \n",
              "4  my_librispeech/LibriSpeech/test-clean/8463/294...  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ Íµ¨Ï∂ï Î∞è ÏÉòÌîåÎßÅ\n",
        "from libri_dataframe import build_librispeech_dataframe\n",
        "\n",
        "print(\"=== Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ Íµ¨Ï∂ï Î∞è ÏÉòÌîåÎßÅ ===\")\n",
        "dataframe = build_librispeech_dataframe(\n",
        "    librispeech_root=LIBRISPEECH_ROOT,\n",
        "    subset=LIBRISPEECH_SUBSET,\n",
        ")\n",
        "\n",
        "# [ÌïµÏã¨] Îã§ÏñëÌïú ÌôîÏûêÍ∞Ä ÏÑûÏù¥ÎèÑÎ°ù ÎûúÎç§ ÏÉòÌîåÎßÅ (100Í∞ú)\n",
        "dataframe_sampled = dataframe.sample(n=100, random_state=42).reset_index(drop=True)\n",
        "print(f\"‚úÖ ÏÉòÌîåÎßÅ ÏôÑÎ£å: 100Í∞ú ÌååÏùº (ÌôîÏûê Ïàò: {len(dataframe_sampled['speaker_id'].unique())}Î™Ö)\")\n",
        "dataframe_sampled.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Ïò§ÎîîÏò§ ÏÉùÏÑ± (XTTS-v2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ìó¨Ìçº Ìï®ÏàòÎì§\n",
        "def resolve_path(path_str): \n",
        "    return str(Path(path_str).resolve())\n",
        "\n",
        "def normalize_text(text):\n",
        "    text = text.lower().strip()\n",
        "    if not text.endswith(('.', '!', '?', ',')): \n",
        "        text += '.'\n",
        "    return text\n",
        "\n",
        "def convert_to_wav(input_path, output_path):\n",
        "    try:\n",
        "        data, samplerate = sf.read(input_path)\n",
        "        sf.write(output_path, data, samplerate)\n",
        "    except: \n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== XTTS-v2 Î™®Îç∏ Î°úÎî© ===\n",
            " > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/mnt/fr20tb/jihwan/.conda/envs/tts/lib/python3.10/site-packages/TTS/api.py:70: UserWarning: `gpu` will be deprecated. Please use `tts.to(device)` instead.\n",
            "  warnings.warn(\"`gpu` will be deprecated. Please use `tts.to(device)` instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " > Using model: xtts\n",
            "‚úÖ XTTS-v2 Î™®Îç∏ Î°úÎìú ÏôÑÎ£å\n"
          ]
        }
      ],
      "source": [
        "# XTTS-v2 Î™®Îç∏ Ï¥àÍ∏∞Ìôî\n",
        "print(\"=== XTTS-v2 Î™®Îç∏ Î°úÎî© ===\")\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = CUDA_DEVICE\n",
        "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=True)\n",
        "print(\"‚úÖ XTTS-v2 Î™®Îç∏ Î°úÎìú ÏôÑÎ£å\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Ï§ÄÎπÑ ÏôÑÎ£å: 100Í∞ú ÏÉòÌîå\n"
          ]
        }
      ],
      "source": [
        "# ÌîÑÎ°¨ÌîÑÌä∏ Ï§ÄÎπÑ Î∞è ÏÉùÏÑ± Î¶¨Ïä§Ìä∏ÏóÖ\n",
        "input_data = []\n",
        "\n",
        "for idx, row in enumerate(dataframe_sampled.to_dict('records')):\n",
        "    spk_id = row['speaker_id']\n",
        "    output_path = OUTPUT_DIR / f\"gen_random_{idx:03d}_{spk_id}.wav\"\n",
        "    \n",
        "    # ÌîÑÎ°¨ÌîÑÌä∏Ïö© WAV Î≥ÄÌôò (ÏóÜÏúºÎ©¥ ÏÉùÏÑ±)\n",
        "    prompt_wav = PROMPT_WAV_DIR / f\"prompt_{spk_id}.wav\"\n",
        "    if not prompt_wav.exists():\n",
        "        convert_to_wav(resolve_path(row['audio_path']), str(prompt_wav))\n",
        "\n",
        "    input_data.append({\n",
        "        \"idx\": idx,\n",
        "        \"original_path\": resolve_path(row['audio_path']),\n",
        "        \"output_path\": str(output_path),\n",
        "        \"text\": row['transcript'],\n",
        "        \"prompt_wav\": str(prompt_wav)\n",
        "    })\n",
        "\n",
        "print(f\"‚úÖ Ï§ÄÎπÑ ÏôÑÎ£å: {len(input_data)}Í∞ú ÏÉòÌîå\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Ïò§ÎîîÏò§ ÏÉùÏÑ± ÌôïÏù∏ (XTTS-v2) ===\n",
            "Generating gen_random_000_7729.wav...\n",
            " > Text splitted to sentences.\n",
            "['all the territorial dignitaries were present governor shannon presided john calhoun the surveyor general made the principal speech a denunciation of the abolitionists supporting the topeka movement chief justice lecompte dignified the occasion with approving remarks.']\n",
            "[!] Warning: The text length exceeds the character limit of 250 for language 'en', this might cause truncated audio.\n",
            " > Processing time: 11.043159008026123\n",
            " > Real-time factor: 0.6054363491242393\n",
            "Generating gen_random_001_1221.wav...\n",
            " > Text splitted to sentences.\n",
            "['this outward mutability indicated and did not more than fairly express the various properties of her inner life.']\n",
            " > Processing time: 3.7742600440979004\n",
            " > Real-time factor: 0.48228114263072963\n",
            "Generating gen_random_002_4507.wav...\n",
            " > Text splitted to sentences.\n",
            "['he must descend with his heart full of charity and severity at the same time as a brother and as a judge to those impenetrable casemates where crawl pell mell those who bleed and those who deal the blow those who weep and those who curse those who fast and those who devour those who endure evil and those who inflict it.']\n",
            "[!] Warning: The text length exceeds the character limit of 250 for language 'en', this might cause truncated audio.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPath(item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_path\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mtts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtts_to_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprompt_wav\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError generating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/mnt/fr20tb/jihwan/.conda/envs/tts/lib/python3.10/site-packages/TTS/api.py:334\u001b[0m, in \u001b[0;36mTTS.tts_to_file\u001b[0;34m(self, text, speaker, language, speaker_wav, emotion, speed, pipe_out, file_path, split_sentences, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Convert text to speech.\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \n\u001b[1;32m    305\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m        Additional arguments for the model.\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_arguments(speaker\u001b[38;5;241m=\u001b[39mspeaker, language\u001b[38;5;241m=\u001b[39mlanguage, speaker_wav\u001b[38;5;241m=\u001b[39mspeaker_wav, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 334\u001b[0m wav \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeaker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeaker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_sentences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_sentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msynthesizer\u001b[38;5;241m.\u001b[39msave_wav(wav\u001b[38;5;241m=\u001b[39mwav, path\u001b[38;5;241m=\u001b[39mfile_path, pipe_out\u001b[38;5;241m=\u001b[39mpipe_out)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file_path\n",
            "File \u001b[0;32m/mnt/fr20tb/jihwan/.conda/envs/tts/lib/python3.10/site-packages/TTS/api.py:276\u001b[0m, in \u001b[0;36mTTS.tts\u001b[0;34m(self, text, speaker, language, speaker_wav, emotion, speed, split_sentences, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Convert text to speech.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m        Additional arguments for the model.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_arguments(\n\u001b[1;32m    274\u001b[0m     speaker\u001b[38;5;241m=\u001b[39mspeaker, language\u001b[38;5;241m=\u001b[39mlanguage, speaker_wav\u001b[38;5;241m=\u001b[39mspeaker_wav, emotion\u001b[38;5;241m=\u001b[39memotion, speed\u001b[38;5;241m=\u001b[39mspeed, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    275\u001b[0m )\n\u001b[0;32m--> 276\u001b[0m wav \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynthesizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeaker_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeaker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlanguage_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreference_wav\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstyle_wav\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstyle_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreference_speaker_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_sentences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_sentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wav\n",
            "File \u001b[0;32m/mnt/fr20tb/jihwan/.conda/envs/tts/lib/python3.10/site-packages/TTS/utils/synthesizer.py:386\u001b[0m, in \u001b[0;36mSynthesizer.tts\u001b[0;34m(self, text, speaker_name, language_name, speaker_wav, style_wav, style_text, reference_wav, reference_speaker_name, split_sentences, **kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sen \u001b[38;5;129;01min\u001b[39;00m sens:\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtts_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msynthesize\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 386\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtts_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynthesize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtts_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m            \u001b[49m\u001b[43mspeaker_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeaker_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvoice_dirs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvoice_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m            \u001b[49m\u001b[43md_vector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeaker_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m            \u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanguage_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m         \u001b[38;5;66;03m# synthesize voice\u001b[39;00m\n\u001b[1;32m    398\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m synthesis(\n\u001b[1;32m    399\u001b[0m             model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtts_model,\n\u001b[1;32m    400\u001b[0m             text\u001b[38;5;241m=\u001b[39msen,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    408\u001b[0m             language_id\u001b[38;5;241m=\u001b[39mlanguage_id,\n\u001b[1;32m    409\u001b[0m         )\n",
            "File \u001b[0;32m/mnt/fr20tb/jihwan/.conda/envs/tts/lib/python3.10/site-packages/TTS/tts/models/xtts.py:419\u001b[0m, in \u001b[0;36mXtts.synthesize\u001b[0;34m(self, text, config, speaker_wav, language, speaker_id, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference(text, language, gpt_cond_latent, speaker_embedding, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msettings)\n\u001b[1;32m    413\u001b[0m settings\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt_cond_len\u001b[39m\u001b[38;5;124m\"\u001b[39m: config\u001b[38;5;241m.\u001b[39mgpt_cond_len,\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt_cond_chunk_len\u001b[39m\u001b[38;5;124m\"\u001b[39m: config\u001b[38;5;241m.\u001b[39mgpt_cond_chunk_len,\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_ref_len\u001b[39m\u001b[38;5;124m\"\u001b[39m: config\u001b[38;5;241m.\u001b[39mmax_ref_len,\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msound_norm_refs\u001b[39m\u001b[38;5;124m\"\u001b[39m: config\u001b[38;5;241m.\u001b[39msound_norm_refs,\n\u001b[1;32m    418\u001b[0m })\n\u001b[0;32m--> 419\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/mnt/fr20tb/jihwan/.conda/envs/tts/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/mnt/fr20tb/jihwan/.conda/envs/tts/lib/python3.10/site-packages/TTS/tts/models/xtts.py:488\u001b[0m, in \u001b[0;36mXtts.full_inference\u001b[0;34m(self, text, ref_audio_path, language, temperature, length_penalty, repetition_penalty, top_k, top_p, do_sample, gpt_cond_len, gpt_cond_chunk_len, max_ref_len, sound_norm_refs, **hf_generate_kwargs)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;124;03mThis function produces an audio clip of the given text being spoken with the given reference voice.\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;124;03m    Sample rate is 24kHz.\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    480\u001b[0m (gpt_cond_latent, speaker_embedding) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_conditioning_latents(\n\u001b[1;32m    481\u001b[0m     audio_path\u001b[38;5;241m=\u001b[39mref_audio_path,\n\u001b[1;32m    482\u001b[0m     gpt_cond_len\u001b[38;5;241m=\u001b[39mgpt_cond_len,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m     sound_norm_refs\u001b[38;5;241m=\u001b[39msound_norm_refs,\n\u001b[1;32m    486\u001b[0m )\n\u001b[0;32m--> 488\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpt_cond_latent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeaker_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhf_generate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/mnt/fr20tb/jihwan/.conda/envs/tts/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/mnt/fr20tb/jihwan/.conda/envs/tts/lib/python3.10/site-packages/TTS/tts/models/xtts.py:541\u001b[0m, in \u001b[0;36mXtts.inference\u001b[0;34m(self, text, language, gpt_cond_latent, speaker_embedding, temperature, length_penalty, repetition_penalty, top_k, top_p, do_sample, num_beams, speed, enable_text_splitting, **hf_generate_kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    537\u001b[0m     text_tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgpt_max_text_tokens\n\u001b[1;32m    538\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ‚ùó XTTS can only generate text with a maximum of 400 tokens.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 541\u001b[0m     gpt_codes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcond_latents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpt_cond_latent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpt_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_beams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhf_generate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    556\u001b[0m     expected_output_len \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m    557\u001b[0m         [gpt_codes\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpt\u001b[38;5;241m.\u001b[39mcode_stride_len], device\u001b[38;5;241m=\u001b[39mtext_tokens\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    558\u001b[0m     )\n\u001b[1;32m    560\u001b[0m     text_len \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([text_tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
            "File \u001b[0;32m/mnt/fr20tb/jihwan/.conda/envs/tts/lib/python3.10/site-packages/TTS/tts/layers/xtts/gpt.py:590\u001b[0m, in \u001b[0;36mGPT.generate\u001b[0;34m(self, cond_latents, text_inputs, **hf_generate_kwargs)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate\u001b[39m(\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    585\u001b[0m     cond_latents,\n\u001b[1;32m    586\u001b[0m     text_inputs,\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhf_generate_kwargs,\n\u001b[1;32m    588\u001b[0m ):\n\u001b[1;32m    589\u001b[0m     gpt_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_embeddings(cond_latents, text_inputs)\n\u001b[0;32m--> 590\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpt_inference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgpt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_audio_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop_audio_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop_audio_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_gen_mel_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgpt_inputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhf_generate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_dict_in_generate\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m hf_generate_kwargs:\n\u001b[1;32m    599\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m gen\u001b[38;5;241m.\u001b[39msequences[:, gpt_inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] :], gen\n",
            "File \u001b[0;32m/mnt/fr20tb/jihwan/.conda/envs/tts/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/mnt/fr20tb/jihwan/.conda/envs/tts/lib/python3.10/site-packages/transformers/generation/utils.py:1622\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1614\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1615\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1616\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1617\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1618\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1619\u001b[0m     )\n\u001b[1;32m   1621\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1622\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1623\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1637\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1638\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1639\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1640\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1646\u001b[0m     )\n",
            "File \u001b[0;32m/mnt/fr20tb/jihwan/.conda/envs/tts/lib/python3.10/site-packages/transformers/generation/utils.py:2786\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2783\u001b[0m unfinished_sequences \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(batch_size, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2784\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(cur_len, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 2786\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_has_unfinished_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis_peer_finished\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   2787\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[1;32m   2788\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2790\u001b[0m     \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n",
            "File \u001b[0;32m/mnt/fr20tb/jihwan/.conda/envs/tts/lib/python3.10/site-packages/transformers/generation/utils.py:1837\u001b[0m, in \u001b[0;36mGenerationMixin._has_unfinished_sequences\u001b[0;34m(self, this_peer_finished, synced_gpus, device)\u001b[0m\n\u001b[1;32m   1835\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m this_peer_finished_flag\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   1836\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1837\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m this_peer_finished:\n\u001b[1;32m   1838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1839\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Ïò§ÎîîÏò§ ÏÉùÏÑ± (XTTS-v2)\n",
        "print(\"=== Ïò§ÎîîÏò§ ÏÉùÏÑ± ÌôïÏù∏ (XTTS-v2) ===\")\n",
        "for item in input_data:\n",
        "    if not os.path.exists(item['output_path']):\n",
        "        # ÌååÏùºÏù¥ ÏóÜÏùÑ ÎïåÎßå ÏÉùÏÑ± ÏàòÌñâ\n",
        "        print(f\"Generating {Path(item['output_path']).name}...\")\n",
        "        try:\n",
        "            tts.tts_to_file(\n",
        "                text=normalize_text(item['text']),\n",
        "                file_path=item['output_path'],\n",
        "                speaker_wav=item['prompt_wav'],\n",
        "                language=\"en\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating {item['output_path']}: {e}\")\n",
        "\n",
        "print(\"‚úÖ Ïò§ÎîîÏò§ ÏÉùÏÑ± ÏôÑÎ£å!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. ÏãúÍ∞ÅÌôî Ìï®Ïàò Ï†ïÏùò\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_and_save(X_embedded, y, filenames, labels, title, filename_suffix):\n",
        "    \"\"\"Real vs Generated ÏãúÍ∞ÅÌôî (ÌôîÏÇ¥Ìëú Ìè¨Ìï®)\"\"\"\n",
        "    plt.figure(figsize=(14, 11))\n",
        "    \n",
        "    target_groups = [\"Original (Real)\", \"Gen (XTTS-v2)\"]\n",
        "    colors = ['blue', 'green']\n",
        "    markers = ['o', '^']\n",
        "\n",
        "    # Ï†ê Ï∞çÍ∏∞\n",
        "    for i, group_name in enumerate(target_groups):\n",
        "        mask = (y == i)\n",
        "        plt.scatter(\n",
        "            X_embedded[mask, 0], X_embedded[mask, 1],\n",
        "            c=colors[i], label=group_name, marker=markers[i],\n",
        "            s=100, alpha=0.7, edgecolors='white', zorder=2\n",
        "        )\n",
        "\n",
        "    # ÌôîÏÇ¥Ìëú Í∑∏Î¶¨Í∏∞ (Real ‚Üí Gen)\n",
        "    unique_names = sorted(list(set(filenames)))\n",
        "    for name in unique_names:\n",
        "        indices_real = [i for i, (f, l) in enumerate(zip(filenames, labels)) if f == name and l == 0]\n",
        "        indices_gen = [i for i, (f, l) in enumerate(zip(filenames, labels)) if f == name and l == 1]\n",
        "        \n",
        "        if indices_real and indices_gen:\n",
        "            start = X_embedded[indices_real[0]]\n",
        "            end = X_embedded[indices_gen[0]]\n",
        "            plt.annotate(\"\", xy=end, xytext=start,\n",
        "                         arrowprops=dict(arrowstyle=\"-|>\", color='gray', alpha=0.3, linewidth=1),\n",
        "                         zorder=1)\n",
        "\n",
        "    plt.title(title, fontsize=16, fontweight='bold')\n",
        "    plt.legend(fontsize=12)\n",
        "    plt.grid(True, alpha=0.3, linestyle='--')\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    save_path = FIGURE_DIR / f\"{filename_suffix}.png\"\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    print(f\"‚úÖ Ï†ÄÏû• ÏôÑÎ£å: {save_path}\")\n",
        "    plt.show()\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Î∂ÑÏÑù 1: Gradient Field (t-SNE & UMAP)\n",
        "\n",
        "Mel SpectrogramÏóêÏÑú GradientÎ•º Ï∂îÏ∂úÌïòÏó¨ Real vs Generated ÏùåÏÑ±ÏùÑ ÎπÑÍµêÌï©ÎãàÎã§.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_gradient_features(audio_path):\n",
        "    \"\"\"Ïò§ÎîîÏò§ÏóêÏÑú Mel Spectrogram Gradient ÌäπÏßï Ï∂îÏ∂ú\"\"\"\n",
        "    try:\n",
        "        waveform, sr = torchaudio.load(audio_path)\n",
        "        # Mel Spectrogram\n",
        "        mel = torchaudio.transforms.MelSpectrogram(\n",
        "            sample_rate=sr, n_fft=1024, hop_length=256, n_mels=80, normalized=True\n",
        "        )(waveform)\n",
        "        mel_db = torchaudio.functional.amplitude_to_DB(mel, multiplier=10., amin=1e-10, db_multiplier=1.)\n",
        "        \n",
        "        # Resize to fixed width (256)\n",
        "        target_width = 256\n",
        "        n_mels, width = mel_db.squeeze(0).shape\n",
        "        if width > target_width:\n",
        "            mel_db = mel_db[:, :, :target_width]\n",
        "        else:\n",
        "            pad = target_width - width\n",
        "            mel_db = torch.nn.functional.pad(mel_db, (0, pad))\n",
        "            \n",
        "        # Gradient Calculation\n",
        "        mel_np = mel_db.squeeze(0).numpy()\n",
        "        grad_y, grad_x = np.gradient(mel_np)\n",
        "        grad_mag = np.sqrt(grad_x**2 + grad_y**2)\n",
        "        return grad_mag.flatten()  # Flatten vector\n",
        "    except:\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gradient Feature Ï∂îÏ∂ú\n",
        "print(\"[Analysis 1] Gradient Field Feature Extraction...\")\n",
        "\n",
        "features = []\n",
        "labels = []\n",
        "filenames = []\n",
        "\n",
        "for item in input_data:\n",
        "    # Real\n",
        "    f = get_gradient_features(item['original_path'])\n",
        "    if f is not None:\n",
        "        features.append(f)\n",
        "        labels.append(0)\n",
        "        filenames.append(f\"s_{item['idx']}\")\n",
        "    # Gen\n",
        "    if os.path.exists(item['output_path']):\n",
        "        f = get_gradient_features(item['output_path'])\n",
        "        if f is not None:\n",
        "            features.append(f)\n",
        "            labels.append(1)\n",
        "            filenames.append(f\"s_{item['idx']}\")\n",
        "\n",
        "X_grad = np.array(features)\n",
        "y_grad = np.array(labels)\n",
        "scaler = StandardScaler()\n",
        "X_grad_scaled = scaler.fit_transform(X_grad)\n",
        "\n",
        "print(f\"‚úÖ Gradient Data Shape: {X_grad.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gradient t-SNE\n",
        "print(\"Running Gradient t-SNE...\")\n",
        "tsne = TSNE(n_components=2, perplexity=30, random_state=42, init='pca', learning_rate='auto')\n",
        "X_tsne = tsne.fit_transform(X_grad_scaled)\n",
        "plot_and_save(X_tsne, y_grad, filenames, labels,\n",
        "              \"t-SNE (Gradient Field): Real vs Gen Flow\", \n",
        "              \"Gradient_Field_tSNE\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gradient UMAP\n",
        "print(\"Running Gradient UMAP...\")\n",
        "reducer = umap.UMAP(n_neighbors=15, min_dist=0.3, random_state=42)\n",
        "X_umap_grad = reducer.fit_transform(X_grad_scaled)\n",
        "plot_and_save(X_umap_grad, y_grad, filenames, labels,\n",
        "              \"UMAP (Gradient Field): Real vs Gen Flow\", \n",
        "              \"Gradient_Field_UMAP\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Î∂ÑÏÑù 2: Whisper Encoder (UMAP)\n",
        "\n",
        "Whisper Î™®Îç∏Ïùò encoder featuresÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Real vs Generated ÏùåÏÑ±ÏùÑ ÎπÑÍµêÌï©ÎãàÎã§.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Whisper Î™®Îç∏ Î°úÎìú\n",
        "print(\"[Analysis 2] Whisper Feature Extraction...\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "whisper_model = WhisperModel.from_pretrained(\"openai/whisper-base\").to(device)\n",
        "whisper_processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
        "print(f\"‚úÖ Whisper Î™®Îç∏ Î°úÎìú ÏôÑÎ£å (device: {device})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_whisper_features(audio_path):\n",
        "    \"\"\"Whisper encoderÎ°ú Ïò§ÎîîÏò§ ÌäπÏßï Ï∂îÏ∂ú\"\"\"\n",
        "    try:\n",
        "        audio, _ = librosa.load(str(audio_path), sr=16000, mono=True)\n",
        "        if len(audio) > 30*16000: \n",
        "            audio = audio[:30*16000]  # 30Ï¥à Ï†úÌïú\n",
        "        \n",
        "        inputs = whisper_processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
        "        input_features = inputs.input_features.to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = whisper_model.encoder(input_features)\n",
        "        \n",
        "        # Mean Pooling\n",
        "        return outputs.last_hidden_state.mean(dim=1).squeeze(0).cpu().numpy()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Whisper Feature Ï∂îÏ∂ú\n",
        "features_w = []\n",
        "labels_w = []\n",
        "filenames_w = []\n",
        "\n",
        "for item in input_data:\n",
        "    # Real\n",
        "    f = get_whisper_features(item['original_path'])\n",
        "    if f is not None:\n",
        "        features_w.append(f)\n",
        "        labels_w.append(0)\n",
        "        filenames_w.append(f\"s_{item['idx']}\")\n",
        "    # Gen\n",
        "    if os.path.exists(item['output_path']):\n",
        "        f = get_whisper_features(item['output_path'])\n",
        "        if f is not None:\n",
        "            features_w.append(f)\n",
        "            labels_w.append(1)\n",
        "            filenames_w.append(f\"s_{item['idx']}\")\n",
        "\n",
        "X_whisper = np.array(features_w)\n",
        "y_whisper = np.array(labels_w)\n",
        "X_whisper_scaled = scaler.fit_transform(X_whisper)\n",
        "\n",
        "print(f\"‚úÖ Whisper Data Shape: {X_whisper.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Whisper UMAP\n",
        "print(\"Running Whisper UMAP...\")\n",
        "reducer_w = umap.UMAP(n_neighbors=15, min_dist=0.3, random_state=42)\n",
        "X_umap_whisper = reducer_w.fit_transform(X_whisper_scaled)\n",
        "plot_and_save(X_umap_whisper, y_whisper, filenames_w, labels_w,\n",
        "              \"UMAP (Whisper Features): Real vs Gen Flow\", \n",
        "              \"Whisper_Encoder_UMAP\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üéâ Î™®Îì† Î∂ÑÏÑù ÏôÑÎ£å!\")\n",
        "print(f\"üìÅ Í≤∞Í≥º Ï†ÄÏû• Ìè¥Îçî: {FIGURE_DIR}\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tts",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
